{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df:\n",
      "                   room_image  \\\n",
      "0     Livingroom\\living_1.jpg   \n",
      "1    Livingroom\\living_10.jpg   \n",
      "2   Livingroom\\living_100.jpg   \n",
      "3  Livingroom\\living_1000.jpg   \n",
      "4  Livingroom\\living_1002.jpg   \n",
      "\n",
      "                                       furniture_arr  \n",
      "0  [furniture_output\\0_67_174_112_175_living_1.pn...  \n",
      "1  [furniture_output\\0_145_185_66_161_living_10.p...  \n",
      "2  [furniture_output\\0_52_203_108_174_living_100....  \n",
      "3  [furniture_output\\0_68_126_113_181_living_1000...  \n",
      "4  [furniture_output\\0_43_73_101_160_living_1002....  \n",
      "\n",
      "furniture_df:\n",
      "                                         furniture  \\\n",
      "0  furniture_output\\0_0_155_136_175_living_937.png   \n",
      "1   furniture_output\\0_0_173_129_223_living_16.png   \n",
      "2  furniture_output\\0_0_17_122_158_living_1319.png   \n",
      "3    furniture_output\\0_0_43_101_199_living_81.png   \n",
      "4  furniture_output\\0_0_45_141_204_living_1318.png   \n",
      "\n",
      "                   from_image  xmin   xmax   ymin   ymax  \n",
      "0   Livingroom\\living_937.jpg   0.0  155.0  136.0  175.0  \n",
      "1    Livingroom\\living_16.jpg   0.0  173.0  129.0  223.0  \n",
      "2  Livingroom\\living_1319.jpg   0.0   17.0  122.0  158.0  \n",
      "3    Livingroom\\living_81.jpg   0.0   43.0  101.0  199.0  \n",
      "4  Livingroom\\living_1318.jpg   0.0   45.0  141.0  204.0  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "coords = True\n",
    "\n",
    "rooms_folder = \"Livingroom\"\n",
    "furniture_folder = \"furniture_output\"\n",
    "\n",
    "def list_images(folder):\n",
    "    return [f for f in os.listdir(folder) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "room_files = list_images(rooms_folder)\n",
    "\n",
    "room_path_dict = {}\n",
    "for room_file in room_files:\n",
    "    base, ext = os.path.splitext(room_file)\n",
    "    room_path_dict[base.lower()] = os.path.join(rooms_folder, room_file)\n",
    "\n",
    "room_to_furniture = {os.path.splitext(f)[0].lower(): [] for f in room_files}\n",
    "\n",
    "furniture_files = list_images(furniture_folder)\n",
    "\n",
    "furniture_rows = []\n",
    "\n",
    "for f_file in furniture_files:\n",
    "    base_name, ext = os.path.splitext(f_file)\n",
    "    \n",
    "    if coords:\n",
    "        # Expected format: {num}_{x_min}_{x_max}_{y_min}_{y_max}_{room image name}\n",
    "\n",
    "        parts = base_name.split(\"_\", maxsplit=5)\n",
    "        if len(parts) < 6:\n",
    "            print(f\"Skipping file with unexpected format: {f_file}\")\n",
    "            continue\n",
    "        num, x_min, x_max, y_min, y_max, room_img_name = parts\n",
    "        \n",
    "        # Re-append the original extension to the room image \n",
    "        room_img_name = room_img_name + ext\n",
    "    else:\n",
    "        \n",
    "        parts = base_name.split(\"_\", maxsplit=1)\n",
    "        if len(parts) < 2:\n",
    "            print(f\"Skipping file with unexpected format: {f_file}\")\n",
    "            continue\n",
    "        num, room_img_name = parts\n",
    "        room_img_name = room_img_name + ext\n",
    "        x_min = x_max = y_min = y_max = None\n",
    "    \n",
    "\n",
    "    furniture_path = os.path.join(furniture_folder, f_file)\n",
    "    \n",
    "   \n",
    "    room_base = os.path.splitext(room_img_name)[0].lower()\n",
    "    \n",
    "  \n",
    "    if room_base in room_path_dict:\n",
    "     \n",
    "        room_to_furniture[room_base].append(furniture_path)\n",
    "        \n",
    "        # Create a row \n",
    "        furniture_rows.append({\n",
    "            \"furniture\": furniture_path,\n",
    "            \"from_image\": room_path_dict[room_base],\n",
    "            \"xmin\": float(x_min) if x_min is not None else None,\n",
    "            \"xmax\": float(x_max) if x_max is not None else None,\n",
    "            \"ymin\": float(y_min) if y_min is not None else None,\n",
    "            \"ymax\": float(y_max) if y_max is not None else None,\n",
    "        })\n",
    "    else:\n",
    "        print(f\"Room image with base name '{room_img_name}' for furniture file {f_file} not found in {rooms_folder}\")\n",
    "\n",
    "# Create the df DataFrame\n",
    "df_rows = []\n",
    "for room_file in room_files:\n",
    "    base, ext = os.path.splitext(room_file)\n",
    "    base_lower = base.lower()\n",
    "    df_rows.append({\n",
    "        \"room_image\": os.path.join(rooms_folder, room_file),\n",
    "        \"furniture_arr\": room_to_furniture.get(base_lower, [])\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(df_rows)\n",
    "furniture_df = pd.DataFrame(furniture_rows)\n",
    "\n",
    "\n",
    "print(\"df:\")\n",
    "print(df.head())\n",
    "print(\"\\nfurniture_df:\")\n",
    "print(furniture_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>furniture</th>\n",
       "      <th>from_image</th>\n",
       "      <th>xmin</th>\n",
       "      <th>xmax</th>\n",
       "      <th>ymin</th>\n",
       "      <th>ymax</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>furniture_output\\0_0_155_136_175_living_937.png</td>\n",
       "      <td>Livingroom\\living_937.jpg</td>\n",
       "      <td>0.0</td>\n",
       "      <td>155.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>175.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>furniture_output\\0_0_173_129_223_living_16.png</td>\n",
       "      <td>Livingroom\\living_16.jpg</td>\n",
       "      <td>0.0</td>\n",
       "      <td>173.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>223.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>furniture_output\\0_0_17_122_158_living_1319.png</td>\n",
       "      <td>Livingroom\\living_1319.jpg</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>158.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>furniture_output\\0_0_43_101_199_living_81.png</td>\n",
       "      <td>Livingroom\\living_81.jpg</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>199.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>furniture_output\\0_0_45_141_204_living_1318.png</td>\n",
       "      <td>Livingroom\\living_1318.jpg</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>204.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4672</th>\n",
       "      <td>furniture_output\\9_131_182_199_223_living_671.png</td>\n",
       "      <td>Livingroom\\living_671.jpg</td>\n",
       "      <td>131.0</td>\n",
       "      <td>182.0</td>\n",
       "      <td>199.0</td>\n",
       "      <td>223.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4673</th>\n",
       "      <td>furniture_output\\9_1_39_151_223_living_1174.png</td>\n",
       "      <td>Livingroom\\living_1174.jpg</td>\n",
       "      <td>1.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>223.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4674</th>\n",
       "      <td>furniture_output\\9_43_63_165_200_living_539.png</td>\n",
       "      <td>Livingroom\\living_539.jpg</td>\n",
       "      <td>43.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4675</th>\n",
       "      <td>furniture_output\\9_53_81_106_129_living_483.png</td>\n",
       "      <td>Livingroom\\living_483.jpg</td>\n",
       "      <td>53.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>129.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4676</th>\n",
       "      <td>furniture_output\\9_85_113_144_172_living_844.png</td>\n",
       "      <td>Livingroom\\living_844.jpg</td>\n",
       "      <td>85.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>172.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4677 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              furniture  \\\n",
       "0       furniture_output\\0_0_155_136_175_living_937.png   \n",
       "1        furniture_output\\0_0_173_129_223_living_16.png   \n",
       "2       furniture_output\\0_0_17_122_158_living_1319.png   \n",
       "3         furniture_output\\0_0_43_101_199_living_81.png   \n",
       "4       furniture_output\\0_0_45_141_204_living_1318.png   \n",
       "...                                                 ...   \n",
       "4672  furniture_output\\9_131_182_199_223_living_671.png   \n",
       "4673    furniture_output\\9_1_39_151_223_living_1174.png   \n",
       "4674    furniture_output\\9_43_63_165_200_living_539.png   \n",
       "4675    furniture_output\\9_53_81_106_129_living_483.png   \n",
       "4676   furniture_output\\9_85_113_144_172_living_844.png   \n",
       "\n",
       "                      from_image   xmin   xmax   ymin   ymax  \n",
       "0      Livingroom\\living_937.jpg    0.0  155.0  136.0  175.0  \n",
       "1       Livingroom\\living_16.jpg    0.0  173.0  129.0  223.0  \n",
       "2     Livingroom\\living_1319.jpg    0.0   17.0  122.0  158.0  \n",
       "3       Livingroom\\living_81.jpg    0.0   43.0  101.0  199.0  \n",
       "4     Livingroom\\living_1318.jpg    0.0   45.0  141.0  204.0  \n",
       "...                          ...    ...    ...    ...    ...  \n",
       "4672   Livingroom\\living_671.jpg  131.0  182.0  199.0  223.0  \n",
       "4673  Livingroom\\living_1174.jpg    1.0   39.0  151.0  223.0  \n",
       "4674   Livingroom\\living_539.jpg   43.0   63.0  165.0  200.0  \n",
       "4675   Livingroom\\living_483.jpg   53.0   81.0  106.0  129.0  \n",
       "4676   Livingroom\\living_844.jpg   85.0  113.0  144.0  172.0  \n",
       "\n",
       "[4677 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "furniture_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"furniture_arr\"].map(lambda x: len(x) > 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train df size: 1003\n",
      "Test df size: 251\n",
      "Train furniture_df size: 3771\n",
      "Test furniture_df size: 906\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Split df into training (80%) and testing (20%) sets.\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "train_room_paths = set(train_df[\"room_image\"])\n",
    "test_room_paths = set(test_df[\"room_image\"])\n",
    "train_furniture_df = furniture_df[furniture_df[\"from_image\"].isin(train_room_paths)]\n",
    "\n",
    "\n",
    "test_furniture_df = furniture_df[furniture_df[\"from_image\"].isin(test_room_paths)]\n",
    "print(\"Train df size:\", len(train_df))\n",
    "print(\"Test df size:\", len(test_df))\n",
    "print(\"Train furniture_df size:\", len(train_furniture_df))\n",
    "print(\"Test furniture_df size:\", len(test_furniture_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "img_size = 224  \n",
    "transform = transforms.Compose([\n",
    "   transforms.Resize((img_size, img_size)),\n",
    "   transforms.ToTensor(),\n",
    "   transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                        std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "import random\n",
    "from PIL import Image, ImageDraw\n",
    "class RoomFurnitureDataset(Dataset):\n",
    "   def __init__(self, df, furniture_df, transform=None):\n",
    "       \"\"\"\n",
    "       df: DataFrame with columns 'room_image' and 'furniture_arr'\n",
    "       furniture_df: DataFrame with columns 'furniture', 'from_image', 'xmin', 'xmax', 'ymin', 'ymax'\n",
    "       transform: Image transformations (applied after masking)\n",
    "       \"\"\"\n",
    "       self.df = df.reset_index(drop=True)\n",
    "       self.furniture_df = furniture_df.reset_index(drop=True)\n",
    "       self.transform = transform\n",
    "\n",
    "   def __len__(self):\n",
    "       return len(self.df)\n",
    "  \n",
    "   def __getitem__(self, idx):\n",
    "       # Loading\n",
    "       room_path = self.df.loc[idx, \"room_image\"]\n",
    "       room_img = Image.open(room_path).convert(\"RGB\")\n",
    "      \n",
    "  \n",
    "       furniture_list = self.df.loc[idx, \"furniture_arr\"]\n",
    "       pos_furniture_path = random.choice(furniture_list)\n",
    "       pos_img = Image.open(pos_furniture_path).convert(\"RGB\")\n",
    "      \n",
    "  \n",
    "       pos_rows = self.furniture_df[self.furniture_df[\"furniture\"] == pos_furniture_path]\n",
    "       if len(pos_rows) > 0:\n",
    "           pos_row = pos_rows.iloc[0]\n",
    "           x_min, x_max = pos_row[\"xmin\"], pos_row[\"xmax\"]\n",
    "           y_min, y_max = pos_row[\"ymin\"], pos_row[\"ymax\"]\n",
    "          \n",
    "           # Compute the center of the bounding box.\n",
    "           center_x = (x_min + x_max) / 2.0\n",
    "           center_y = (y_min + y_max) / 2.0\n",
    "          \n",
    "           # Compute the max dimension of the bounding box.\n",
    "           box_width = x_max - x_min\n",
    "           box_height = y_max - y_min\n",
    "           max_dim = max(box_width, box_height)\n",
    "          \n",
    "          \n",
    "           multiplier = random.uniform(1.0, 1.5)\n",
    "           mask_size = multiplier * max_dim\n",
    "          \n",
    "        \n",
    "           img_width, img_height = room_img.size\n",
    "          \n",
    "          \n",
    "           left = center_x - mask_size/2.0\n",
    "           top = center_y - mask_size/2.0\n",
    "           right = center_x + mask_size/2.0\n",
    "           bottom = center_y + mask_size/2.0\n",
    "          \n",
    "           \n",
    "           left = max(0, left)\n",
    "           top = max(0, top)\n",
    "           right = min(img_width, right)\n",
    "           bottom = min(img_height, bottom)\n",
    "          \n",
    "          \n",
    "           draw = ImageDraw.Draw(room_img)\n",
    "           draw.rectangle([left, top, right, bottom], fill=\"black\")\n",
    "       else:\n",
    "           \n",
    "           print(f\"Warning: No bounding box found for {pos_furniture_path}\")\n",
    "      \n",
    "   \n",
    "       if self.transform:\n",
    "           room_img = self.transform(room_img)\n",
    "      \n",
    "    \n",
    "       if self.transform:\n",
    "           pos_img = self.transform(pos_img)\n",
    "      \n",
    "       \n",
    "       room_id = self.df.loc[idx, \"room_image\"]\n",
    "       neg_candidates = self.furniture_df[self.furniture_df[\"from_image\"] != room_id]\n",
    "       neg_row = neg_candidates.sample(1).iloc[0]\n",
    "       neg_furniture_path = neg_row[\"furniture\"]\n",
    "       neg_img = Image.open(neg_furniture_path).convert(\"RGB\")\n",
    "       if self.transform:\n",
    "           neg_img = self.transform(neg_img)\n",
    "      \n",
    "       return room_img, pos_img, neg_img\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = RoomFurnitureDataset(train_df, train_furniture_df, transform=transform)\n",
    "test_dataset = RoomFurnitureDataset(test_df, test_furniture_df, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class EmbeddingNet(nn.Module):\n",
    "    def __init__(self, embed_dim=128, lock_base=True):\n",
    "        super(EmbeddingNet, self).__init__()\n",
    "        # pretrained ResNet18\n",
    "        self.base_model = models.resnet18(pretrained=True)\n",
    "        \n",
    "        \n",
    "        if lock_base:\n",
    "            for param in self.base_model.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        num_features = self.base_model.fc.in_features\n",
    "        self.base_model.fc = nn.Identity() \n",
    "        \n",
    "       \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(num_features, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, embed_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.base_model(x)\n",
    "        x = self.fc(x)\n",
    "        # L2\n",
    "        x = nn.functional.normalize(x, p=2, dim=1)\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EmbeddingNet(\n",
       "  (base_model): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Identity()\n",
       "  )\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=256, out_features=128, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "room_model = EmbeddingNet().to(device)\n",
    "furniture_model = EmbeddingNet().to(device)\n",
    "\n",
    "room_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "margin = 1.0\n",
    "triplet_loss = nn.TripletMarginLoss(margin=margin, p=2)\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(list(room_model.parameters()) + list(furniture_model.parameters()), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Batch 0/63: Train Loss = 0.4889 | Batch Acc = 0.8750\n",
      "Epoch 1/10 - Batch 10/63: Train Loss = 0.5754 | Batch Acc = 0.6875\n",
      "Epoch 1/10 - Batch 20/63: Train Loss = 0.6460 | Batch Acc = 0.6250\n",
      "Epoch 1/10 - Batch 30/63: Train Loss = 0.5570 | Batch Acc = 0.8125\n",
      "Epoch 1/10 - Batch 40/63: Train Loss = 0.4884 | Batch Acc = 0.7500\n",
      "Epoch 1/10 - Batch 50/63: Train Loss = 0.6987 | Batch Acc = 0.6875\n",
      "Epoch 1/10 - Batch 60/63: Train Loss = 0.6102 | Batch Acc = 0.6875\n",
      "Epoch 1/10 - Train Loss: 0.5064 | Train Acc: 0.8365 | Test Loss: 0.5378 | Test Acc: 0.8167\n",
      "Epoch 2/10 - Batch 0/63: Train Loss = 0.5876 | Batch Acc = 0.8125\n",
      "Epoch 2/10 - Batch 10/63: Train Loss = 0.6327 | Batch Acc = 0.6875\n",
      "Epoch 2/10 - Batch 20/63: Train Loss = 0.5485 | Batch Acc = 0.8750\n",
      "Epoch 2/10 - Batch 30/63: Train Loss = 0.2668 | Batch Acc = 1.0000\n",
      "Epoch 2/10 - Batch 40/63: Train Loss = 0.4735 | Batch Acc = 0.8125\n",
      "Epoch 2/10 - Batch 50/63: Train Loss = 0.4632 | Batch Acc = 0.8750\n",
      "Epoch 2/10 - Batch 60/63: Train Loss = 0.5739 | Batch Acc = 0.7500\n",
      "Epoch 2/10 - Train Loss: 0.5066 | Train Acc: 0.8255 | Test Loss: 0.5232 | Test Acc: 0.8048\n",
      "Epoch 3/10 - Batch 0/63: Train Loss = 0.6176 | Batch Acc = 0.7500\n",
      "Epoch 3/10 - Batch 10/63: Train Loss = 0.5561 | Batch Acc = 0.6875\n",
      "Epoch 3/10 - Batch 20/63: Train Loss = 0.6527 | Batch Acc = 0.8125\n",
      "Epoch 3/10 - Batch 30/63: Train Loss = 0.7975 | Batch Acc = 0.5625\n",
      "Epoch 3/10 - Batch 40/63: Train Loss = 0.5453 | Batch Acc = 0.6875\n",
      "Epoch 3/10 - Batch 50/63: Train Loss = 0.3613 | Batch Acc = 0.8750\n",
      "Epoch 3/10 - Batch 60/63: Train Loss = 0.5008 | Batch Acc = 0.8750\n",
      "Epoch 3/10 - Train Loss: 0.5051 | Train Acc: 0.8225 | Test Loss: 0.5710 | Test Acc: 0.7769\n",
      "Epoch 4/10 - Batch 0/63: Train Loss = 0.3099 | Batch Acc = 0.8750\n",
      "Epoch 4/10 - Batch 10/63: Train Loss = 0.3821 | Batch Acc = 0.9375\n",
      "Epoch 4/10 - Batch 20/63: Train Loss = 0.3502 | Batch Acc = 0.9375\n",
      "Epoch 4/10 - Batch 30/63: Train Loss = 0.5261 | Batch Acc = 0.7500\n",
      "Epoch 4/10 - Batch 40/63: Train Loss = 0.5675 | Batch Acc = 0.8750\n",
      "Epoch 4/10 - Batch 50/63: Train Loss = 0.4735 | Batch Acc = 0.8750\n",
      "Epoch 4/10 - Batch 60/63: Train Loss = 0.4804 | Batch Acc = 0.8750\n",
      "Epoch 4/10 - Train Loss: 0.4915 | Train Acc: 0.8156 | Test Loss: 0.6057 | Test Acc: 0.7570\n",
      "Epoch 5/10 - Batch 0/63: Train Loss = 0.4615 | Batch Acc = 0.8750\n",
      "Epoch 5/10 - Batch 10/63: Train Loss = 0.3936 | Batch Acc = 0.8125\n",
      "Epoch 5/10 - Batch 20/63: Train Loss = 0.3956 | Batch Acc = 0.8750\n",
      "Epoch 5/10 - Batch 30/63: Train Loss = 0.5092 | Batch Acc = 0.8125\n",
      "Epoch 5/10 - Batch 40/63: Train Loss = 0.6151 | Batch Acc = 0.8125\n",
      "Epoch 5/10 - Batch 50/63: Train Loss = 0.5085 | Batch Acc = 0.8750\n",
      "Epoch 5/10 - Batch 60/63: Train Loss = 0.5231 | Batch Acc = 0.8125\n",
      "Epoch 5/10 - Train Loss: 0.4942 | Train Acc: 0.8305 | Test Loss: 0.5807 | Test Acc: 0.7689\n",
      "Epoch 6/10 - Batch 0/63: Train Loss = 0.4531 | Batch Acc = 1.0000\n",
      "Epoch 6/10 - Batch 10/63: Train Loss = 0.6322 | Batch Acc = 0.9375\n",
      "Epoch 6/10 - Batch 20/63: Train Loss = 0.5639 | Batch Acc = 0.7500\n",
      "Epoch 6/10 - Batch 30/63: Train Loss = 0.5795 | Batch Acc = 0.7500\n",
      "Epoch 6/10 - Batch 40/63: Train Loss = 0.6549 | Batch Acc = 0.7500\n",
      "Epoch 6/10 - Batch 50/63: Train Loss = 0.7878 | Batch Acc = 0.6250\n",
      "Epoch 6/10 - Batch 60/63: Train Loss = 0.4075 | Batch Acc = 0.9375\n",
      "Epoch 6/10 - Train Loss: 0.4871 | Train Acc: 0.8485 | Test Loss: 0.6149 | Test Acc: 0.7490\n",
      "Epoch 7/10 - Batch 0/63: Train Loss = 0.6658 | Batch Acc = 0.6875\n",
      "Epoch 7/10 - Batch 10/63: Train Loss = 0.4872 | Batch Acc = 0.8750\n",
      "Epoch 7/10 - Batch 20/63: Train Loss = 0.4835 | Batch Acc = 1.0000\n",
      "Epoch 7/10 - Batch 30/63: Train Loss = 0.5673 | Batch Acc = 0.7500\n",
      "Epoch 7/10 - Batch 40/63: Train Loss = 0.5543 | Batch Acc = 0.8125\n",
      "Epoch 7/10 - Batch 50/63: Train Loss = 0.4374 | Batch Acc = 0.8750\n",
      "Epoch 7/10 - Batch 60/63: Train Loss = 0.6568 | Batch Acc = 0.8125\n",
      "Epoch 7/10 - Train Loss: 0.4937 | Train Acc: 0.8325 | Test Loss: 0.5796 | Test Acc: 0.7649\n",
      "Epoch 8/10 - Batch 0/63: Train Loss = 0.6537 | Batch Acc = 0.8125\n",
      "Epoch 8/10 - Batch 10/63: Train Loss = 0.4816 | Batch Acc = 0.8125\n",
      "Epoch 8/10 - Batch 20/63: Train Loss = 0.3820 | Batch Acc = 0.8125\n",
      "Epoch 8/10 - Batch 30/63: Train Loss = 0.6061 | Batch Acc = 0.8750\n",
      "Epoch 8/10 - Batch 40/63: Train Loss = 0.4491 | Batch Acc = 0.8125\n",
      "Epoch 8/10 - Batch 50/63: Train Loss = 0.3557 | Batch Acc = 0.9375\n",
      "Epoch 8/10 - Batch 60/63: Train Loss = 0.2400 | Batch Acc = 1.0000\n",
      "Epoch 8/10 - Train Loss: 0.4876 | Train Acc: 0.8345 | Test Loss: 0.5810 | Test Acc: 0.7649\n",
      "Epoch 9/10 - Batch 0/63: Train Loss = 0.3912 | Batch Acc = 0.9375\n",
      "Epoch 9/10 - Batch 10/63: Train Loss = 0.1929 | Batch Acc = 1.0000\n",
      "Epoch 9/10 - Batch 20/63: Train Loss = 0.6180 | Batch Acc = 0.6250\n",
      "Epoch 9/10 - Batch 30/63: Train Loss = 0.6794 | Batch Acc = 0.7500\n",
      "Epoch 9/10 - Batch 40/63: Train Loss = 0.2124 | Batch Acc = 1.0000\n",
      "Epoch 9/10 - Batch 50/63: Train Loss = 0.4278 | Batch Acc = 1.0000\n",
      "Epoch 9/10 - Batch 60/63: Train Loss = 0.4981 | Batch Acc = 0.7500\n",
      "Epoch 9/10 - Train Loss: 0.4567 | Train Acc: 0.8504 | Test Loss: 0.5415 | Test Acc: 0.7649\n",
      "Epoch 10/10 - Batch 0/63: Train Loss = 0.6656 | Batch Acc = 0.7500\n",
      "Epoch 10/10 - Batch 10/63: Train Loss = 0.5205 | Batch Acc = 0.8125\n",
      "Epoch 10/10 - Batch 20/63: Train Loss = 0.5601 | Batch Acc = 0.7500\n",
      "Epoch 10/10 - Batch 30/63: Train Loss = 0.3878 | Batch Acc = 0.9375\n",
      "Epoch 10/10 - Batch 40/63: Train Loss = 0.5071 | Batch Acc = 0.8750\n",
      "Epoch 10/10 - Batch 50/63: Train Loss = 0.6875 | Batch Acc = 0.7500\n",
      "Epoch 10/10 - Batch 60/63: Train Loss = 0.4512 | Batch Acc = 0.8125\n",
      "Epoch 10/10 - Train Loss: 0.5312 | Train Acc: 0.8096 | Test Loss: 0.5404 | Test Acc: 0.7968\n",
      "Training and testing complete.\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    room_model.train()\n",
    "    furniture_model.train()\n",
    "    train_running_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_samples = 0\n",
    "\n",
    "    for batch_idx, (room_imgs, pos_imgs, neg_imgs) in enumerate(train_loader):\n",
    "        room_imgs = room_imgs.to(device)\n",
    "        pos_imgs = pos_imgs.to(device)\n",
    "        neg_imgs = neg_imgs.to(device)\n",
    "        \n",
    "       \n",
    "        room_embeds = room_model(room_imgs)\n",
    "        pos_embeds = furniture_model(pos_imgs)\n",
    "        neg_embeds = furniture_model(neg_imgs)\n",
    "        \n",
    "        loss = triplet_loss(room_embeds, pos_embeds, neg_embeds)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_running_loss += loss.item()\n",
    "        \n",
    "        \n",
    "        d_pos = torch.norm(room_embeds - pos_embeds, dim=1)\n",
    "        d_neg = torch.norm(room_embeds - neg_embeds, dim=1)\n",
    "        correct = (d_pos < d_neg).sum().item()\n",
    "        train_correct += correct\n",
    "        train_samples += room_imgs.size(0)\n",
    "        \n",
    "        if batch_idx % 10 == 0:\n",
    "            batch_acc = correct / room_imgs.size(0)\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs} - Batch {batch_idx}/{len(train_loader)}: Train Loss = {loss.item():.4f} | Batch Acc = {batch_acc:.4f}\")\n",
    "    \n",
    "    train_epoch_loss = train_running_loss / len(train_loader)\n",
    "    train_epoch_acc = train_correct / train_samples\n",
    "    \n",
    "   \n",
    "    room_model.eval()\n",
    "    furniture_model.eval()\n",
    "    test_running_loss = 0.0\n",
    "    test_correct = 0\n",
    "    test_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (room_imgs, pos_imgs, neg_imgs) in enumerate(test_loader):\n",
    "            room_imgs = room_imgs.to(device)\n",
    "            pos_imgs = pos_imgs.to(device)\n",
    "            neg_imgs = neg_imgs.to(device)\n",
    "            \n",
    "            room_embeds = room_model(room_imgs)\n",
    "            pos_embeds = furniture_model(pos_imgs)\n",
    "            neg_embeds = furniture_model(neg_imgs)\n",
    "            \n",
    "            loss = triplet_loss(room_embeds, pos_embeds, neg_embeds)\n",
    "            test_running_loss += loss.item()\n",
    "            \n",
    "            # Compute distances and accuracy for the batch\n",
    "            d_pos = torch.norm(room_embeds - pos_embeds, dim=1)\n",
    "            d_neg = torch.norm(room_embeds - neg_embeds, dim=1)\n",
    "            correct = (d_pos < d_neg).sum().item()\n",
    "            test_correct += correct\n",
    "            test_samples += room_imgs.size(0)\n",
    "    \n",
    "    test_epoch_loss = test_running_loss / len(test_loader)\n",
    "    test_epoch_acc = test_correct / test_samples\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {train_epoch_loss:.4f} | Train Acc: {train_epoch_acc:.4f} | Test Loss: {test_epoch_loss:.4f} | Test Acc: {test_epoch_acc:.4f}\")\n",
    "\n",
    "print(\"Training and testing complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full models saved successfully!\n"
     ]
    }
   ],
   "source": [
    "torch.save(room_model, \"room_model_837%_312am.pth\")\n",
    "torch.save(room_model, \"furniture_model_837%_312am.pth\")\n",
    "\n",
    "print(\"Full models saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vsrik\\anaconda3\\envs\\aneesh\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\vsrik\\anaconda3\\envs\\aneesh\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Expected state_dict to be dict-like, got <class '__main__.EmbeddingNet'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Initialize the model and load the state dictionary\u001b[39;00m\n\u001b[32m     30\u001b[39m model = EmbeddingNet()\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m model.eval()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vsrik\\anaconda3\\envs\\aneesh\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2513\u001b[39m, in \u001b[36mModule.load_state_dict\u001b[39m\u001b[34m(self, state_dict, strict, assign)\u001b[39m\n\u001b[32m   2476\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\u001b[39;00m\n\u001b[32m   2477\u001b[39m \n\u001b[32m   2478\u001b[39m \u001b[33;03mIf :attr:`strict` is ``True``, then\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2510\u001b[39m \u001b[33;03m    ``RuntimeError``.\u001b[39;00m\n\u001b[32m   2511\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2512\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(state_dict, Mapping):\n\u001b[32m-> \u001b[39m\u001b[32m2513\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m   2514\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected state_dict to be dict-like, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(state_dict)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2515\u001b[39m     )\n\u001b[32m   2517\u001b[39m missing_keys: List[\u001b[38;5;28mstr\u001b[39m] = []\n\u001b[32m   2518\u001b[39m unexpected_keys: List[\u001b[38;5;28mstr\u001b[39m] = []\n",
      "\u001b[31mTypeError\u001b[39m: Expected state_dict to be dict-like, got <class '__main__.EmbeddingNet'>."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "\n",
    "class EmbeddingNet(nn.Module):\n",
    "   def __init__(self, embed_dim=128):\n",
    "       super(EmbeddingNet, self).__init__()\n",
    "       self.base_model = models.resnet18(pretrained=True)\n",
    "       num_features = self.base_model.fc.in_features\n",
    "       self.base_model.fc = nn.Identity()\n",
    "       self.fc = nn.Sequential(\n",
    "           nn.Linear(num_features, 256),\n",
    "           nn.ReLU(),\n",
    "           nn.Linear(256, embed_dim)\n",
    "       )\n",
    "  \n",
    "   def forward(self, x):\n",
    "       x = self.base_model(x)\n",
    "       x = self.fc(x)\n",
    "       x = nn.functional.normalize(x, p=2, dim=1)\n",
    "       return x\n",
    "\n",
    "\n",
    "\n",
    "checkpoint = torch.load('furniture_model_837%_312am.pth', map_location=torch.device('cpu'), weights_only=False)\n",
    "#Initializiing it\n",
    "model = EmbeddingNet()\n",
    "model.load_state_dict(checkpoint)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aneesh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
